# -*- coding: utf-8 -*-
"""190576D_L7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZKGVM_TF4eOvSHtVgF1eWXu6BpQWKDTx
"""

import pandas as pd
import numpy as np

labels = ["label_1", "label_2", "label_3", "label_4"]
empty_label = "label_2"
features = [f'feature_{i}' for i in range(1,769)]

"""Importing data"""

train_df = pd.read_csv('train.csv')
train_df.head()

valid_df = pd.read_csv('valid.csv')
valid_df.head()

test_df = pd.read_csv('test.csv')
test_df.head()

print(train_df.shape)
print(train_df[train_df["label_2"].notna()].shape)

print(valid_df.shape)
print(valid_df[valid_df["label_2"].notna()].shape)

"""The above result shows that label 2 has empty values in 480 and 14 rows in train and valid rows respectively. They will be handled via deletion.

**Label 2**
"""

x_train = train_df[train_df['label_2'].notna()]                                              # NaN values are dropped
x_train.drop(labels, axis=1, inplace=True)
y_train = train_df[train_df['label_2'].notna()]['label_2']

x_valid = valid_df[valid_df['label_2'].notna()]                                             # NaN values are dropped
x_valid.drop(labels, axis=1, inplace=True)
y_valid = valid_df[valid_df['label_2'].notna()]['label_2']

x_test = test_df.copy()
x_test.drop(['ID'], axis=1, inplace=True)

"""Feature scaling using robust scaler"""

from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
x_tr_sc = scaler.fit_transform(x_train)
x_vl_sc = scaler.fit_transform(x_valid)
x_test_sc = scaler.fit_transform(x_test)

"""SVM model"""

from sklearn import svm
from sklearn.metrics import accuracy_score

svmc = svm.SVC(kernel="linear")
svmc.fit(x_tr_sc,y_train)

y_pred_svmc = svmc.predict(x_vl_sc)

acc_vl = accuracy_score(y_valid, y_pred_svmc)
acc_vl

"""K-nearest model"""

from sklearn.neighbors import KNeighborsClassifier

knnc = KNeighborsClassifier(n_neighbors=50)
knnc.fit(x_tr_sc,y_train)

y_pred_knnc = knnc.predict(x_vl_sc)

acc_vl = accuracy_score(y_valid, y_pred_knnc)
acc_vl

"""Above results show that SVM classifier has a higher accuracy than k-nearest classifier with deleting of missing values.

Feature engineering with Principal Component Analysis and deletion of missing values
"""

x_train = train_df[train_df['label_2'].notna()]                                              # NaN values are dropped
x_train.drop(labels, axis=1, inplace=True)
y_train = train_df[train_df['label_2'].notna()]['label_2']

x_valid = valid_df[valid_df['label_2'].notna()]                                             # NaN values are dropped
x_valid.drop(labels, axis=1, inplace=True)
y_valid = valid_df[valid_df['label_2'].notna()]['label_2']

x_test = test_df.copy()
x_test.drop(['ID'], axis=1, inplace=True)

from sklearn.decomposition import PCA

new_feature_numbers = []
accuracy_values = []

n_c = 0.8
while(n_c<0.99):
  pca = PCA(n_components=n_c, svd_solver='full')
  pca.fit(x_train)

  x_train_trf = pd.DataFrame(pca.transform(x_train))
  x_valid_trf = pd.DataFrame(pca.transform(x_valid))
  new_feature_numbers.append(x_train_trf.shape[1])

  classifier = svm.SVC(kernel='linear')
  classifier.fit(x_train_trf,y_train)
  y_pred2 = classifier.predict(x_valid_trf)
  accuracy_values.append(accuracy_score(y_valid,y_pred2))
  n_c=n_c+0.02

print(new_feature_numbers)
print(accuracy_values)

i=0
while(i<len(accuracy_values)):
  print("Accuracy for "+ str(new_feature_numbers[i])+" features :- "+str(accuracy_values[i]))
  i=i+1

"""Using 0.98 as the n_components value;"""

pca = PCA(n_components=0.98, svd_solver='full')
pca.fit(x_train)

x_train_trf = pd.DataFrame(pca.transform(x_train))
x_test_trf = pd.DataFrame(pca.transform(x_test))

classifier = svm.SVC(kernel='linear')
classifier.fit(x_train_trf,y_train)
y_pred = classifier.predict(x_test_trf)
pd.DataFrame(y_pred).to_csv("l2.csv")

x_test_trf

train_df = pd.read_csv('train.csv')
valid_df = pd.read_csv('valid.csv')
test_df = pd.read_csv('test.csv')

x_train = train_df.copy()
x_train.drop(labels, axis=1, inplace=True)
y_train_1 = train_df['label_1']
y_train_3 = train_df['label_3']
y_train_4 = train_df['label_4']

x_valid = valid_df.copy()
x_valid.drop(labels, axis=1, inplace=True)
y_valid_1 = valid_df['label_1']
y_valid_3 = valid_df['label_3']
y_valid_4 = valid_df['label_4']

x_test = test_df.copy()
x_test.drop(['ID'], axis=1, inplace=True)

"""**Label 1**"""

new_feature_numbers = []
accuracy_values = []

n_c = 0.8
while(n_c<0.99):
  pca = PCA(n_components=n_c, svd_solver='full')
  pca.fit(x_train)

  x_train_trf = pd.DataFrame(pca.transform(x_train))
  x_valid_trf = pd.DataFrame(pca.transform(x_valid))
  new_feature_numbers.append(x_train_trf.shape[1])

  classifier = svm.SVC(kernel='linear')
  classifier.fit(x_train_trf,y_train_1)
  y_pred2 = classifier.predict(x_valid_trf)
  accuracy_values.append(accuracy_score(y_valid_1,y_pred2))
  n_c=n_c+0.02

print(new_feature_numbers)
print(accuracy_values)

i=0
while(i<len(accuracy_values)):
  print("Accuracy for "+ str(new_feature_numbers[i])+" features :- "+str(accuracy_values[i]))
  i=i+1

"""Using 0.98 as the n_components value;"""

pca = PCA(n_components=0.98, svd_solver='full')
pca.fit(x_train)

x_train_trf = pd.DataFrame(pca.transform(x_train))
x_test_trf = pd.DataFrame(pca.transform(x_test))

classifier = svm.SVC(kernel='linear')
classifier.fit(x_train_trf,y_train_1)
y_pred = classifier.predict(x_test_trf)
pd.DataFrame(y_pred).to_csv("l1.csv")

"""**Label 3**"""

new_feature_numbers = []
accuracy_values = []

n_c = 0.8
while(n_c<0.99):
  pca = PCA(n_components=n_c, svd_solver='full')
  pca.fit(x_train)

  x_train_trf = pd.DataFrame(pca.transform(x_train))
  x_valid_trf = pd.DataFrame(pca.transform(x_valid))
  new_feature_numbers.append(x_train_trf.shape[1])

  classifier = svm.SVC(kernel='linear')
  classifier.fit(x_train_trf,y_train_3)
  y_pred2 = classifier.predict(x_valid_trf)
  accuracy_values.append(accuracy_score(y_valid_3,y_pred2))
  n_c=n_c+0.02

print(new_feature_numbers)
print(accuracy_values)

i=0
while(i<len(accuracy_values)):
  print("Accuracy for "+ str(new_feature_numbers[i])+" features :- "+str(accuracy_values[i]))
  i=i+1

"""Since accuracy stays constant after *n_components* becomes 0.9, that value, with 127 features are chosen."""

pca = PCA(n_components=0.9, svd_solver='full')
pca.fit(x_train)

x_train_trf = pd.DataFrame(pca.transform(x_train))
x_test_trf = pd.DataFrame(pca.transform(x_test))

classifier = svm.SVC(kernel='linear')
classifier.fit(x_train_trf,y_train_3)
y_pred = classifier.predict(x_test_trf)
pd.DataFrame(y_pred).to_csv("l3.csv")

"""**Label 4**"""

train_df['label_4'].value_counts()

"""Since the distribution is uneven, an additional parameter, *class_weight* with a value 'balanced' is added to the svc classifier."""

new_feature_numbers = []
accuracy_values = []

n_c = 0.8
while(n_c<0.99):
  pca = PCA(n_components=n_c, svd_solver='full')
  pca.fit(x_train)

  x_train_trf = pd.DataFrame(pca.transform(x_train))
  x_valid_trf = pd.DataFrame(pca.transform(x_valid))
  new_feature_numbers.append(x_train_trf.shape[1])

  classifier = svm.SVC(kernel='linear', class_weight="balanced")
  classifier.fit(x_train_trf,y_train_4)
  y_pred2 = classifier.predict(x_valid_trf)
  accuracy_values.append(accuracy_score(y_valid_4,y_pred2))
  n_c=n_c+0.02

print(new_feature_numbers)
print(accuracy_values)

i=0
while(i<len(accuracy_values)):
  print("Accuracy for "+ str(new_feature_numbers[i])+" features :- "+str(accuracy_values[i]))
  i=i+1

"""Using 0.98 as the n_components value;"""

pca = PCA(n_components=0.98, svd_solver='full')
pca.fit(x_train)

x_train_trf = pd.DataFrame(pca.transform(x_train))
x_test_trf = pd.DataFrame(pca.transform(x_test))

classifier = svm.SVC(kernel='linear')
classifier.fit(x_train_trf,y_train_4)
y_pred = classifier.predict(x_test_trf)
pd.DataFrame(y_pred).to_csv("l4.csv")