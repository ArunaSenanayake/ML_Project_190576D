# -*- coding: utf-8 -*-
"""190576D_L12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NrZdT_-FnArRgFywYfc237tsegY7gMAK
"""

import pandas as pd
import numpy as np

labels = ["label_1", "label_2", "label_3", "label_4"]
empty_label = "label_2"
features = [f'feature_{i}' for i in range(1,769)]

train_df = pd.read_csv('train.csv')
train_df.head()

valid_df = pd.read_csv('valid.csv')
valid_df.head()

test_df = pd.read_csv('test.csv')
test_df.head()

print("Checking for null values")
print("Label 1 ;")
print(train_df.shape)
print(train_df[train_df["label_1"].notna()].shape)

print(valid_df.shape)
print(valid_df[valid_df["label_1"].notna()].shape)

print("Label 2 ;")
print(train_df.shape)
print(train_df[train_df["label_2"].notna()].shape)

print(valid_df.shape)
print(valid_df[valid_df["label_2"].notna()].shape)

print("Label 3 ;")
print(train_df.shape)
print(train_df[train_df["label_3"].notna()].shape)

print(valid_df.shape)
print(valid_df[valid_df["label_3"].notna()].shape)

print("Label 4 ;")
print(train_df.shape)
print(train_df[train_df["label_4"].notna()].shape)

print(valid_df.shape)
print(valid_df[valid_df["label_4"].notna()].shape)

"""This shows that there are 480 and 6 empty values for laabel 2 in train and valid datasets respectively. This will be handled via deletion."""

train_df['label_4'].value_counts()

"""Above result proves that the distribution of values in label 4 is uneven. It will be handled in the classifier.

**Data preprocessing**
"""

x_train = train_df.copy()
x_train.drop(labels, axis=1, inplace=True)
y_train_1 = train_df['label_1']
y_train_3 = train_df['label_3']
y_train_4 = train_df['label_4']

x_train_2 = train_df[train_df['label_2'].notna()]                                              # Dropping NaN values for label 2
x_train_2.drop(labels, axis=1, inplace=True)
y_train_2 = train_df[train_df['label_2'].notna()]['label_2']

x_valid = valid_df.copy()
x_valid.drop(labels, axis=1, inplace=True)
y_valid_1 = valid_df['label_1']
y_valid_3 = valid_df['label_3']
y_valid_4 = valid_df['label_4']

x_valid_2 = valid_df[valid_df['label_2'].notna()]                                             # Dropping NaN values for label 2
x_valid_2.drop(labels, axis=1, inplace=True)
y_valid_2 = valid_df[valid_df['label_2'].notna()]['label_2']

x_test = test_df.copy()
x_test.drop(['ID'], axis=1, inplace=True)

"""Feature scaling"""

from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
x_tr_sc = scaler.fit_transform(x_train)
x_vl_sc = scaler.fit_transform(x_valid)
x_test_sc = scaler.fit_transform(x_test)

x_tr_sc_2 = scaler.fit_transform(x_train_2)
x_vl_sc_2 = scaler.fit_transform(x_valid_2)

"""**Label 1**

SVM model
"""

from sklearn import svm
from sklearn.metrics import accuracy_score

svmc = svm.SVC(kernel="linear")
svmc.fit(x_tr_sc,y_train_1)

y_pred_svmc = svmc.predict(x_vl_sc)

acc_vl = accuracy_score(y_valid_1, y_pred_svmc)
acc_vl

"""K-nearest model"""

from sklearn.neighbors import KNeighborsClassifier

knnc = KNeighborsClassifier(n_neighbors=50)
knnc.fit(x_tr_sc,y_train_1)

y_pred_knnc = knnc.predict(x_vl_sc)

acc_vl = accuracy_score(y_valid_1, y_pred_knnc)
acc_vl

"""Above results show that the SVM model produces better results for scaling.

**Label 2**

SVM model
"""

svmc = svm.SVC(kernel="linear")
svmc.fit(x_tr_sc_2,y_train_2)

y_pred_svmc = svmc.predict(x_vl_sc_2)

acc_vl = accuracy_score(y_valid_2, y_pred_svmc)
acc_vl

"""**Label 3**

SVM model
"""

svmc = svm.SVC(kernel="linear")
svmc.fit(x_tr_sc,y_train_3)

y_pred_svmc = svmc.predict(x_vl_sc)

acc_vl = accuracy_score(y_valid_3, y_pred_svmc)
acc_vl

"""**Label 4**

SVM model

An additional parameter, *class_weight* with the value *balanced* was used to resolve the issue with the uneven distribution of label values.
"""

svmc = svm.SVC(kernel="linear", class_weight="balanced")
svmc.fit(x_tr_sc,y_train_4)

y_pred_svmc = svmc.predict(x_vl_sc)

acc_vl = accuracy_score(y_valid_4, y_pred_svmc)
acc_vl

"""**Feature engineering - with deletion for empty values in label 2**"""

x_train = train_df.copy()
x_train.drop(labels, axis=1, inplace=True)
y_train_1 = train_df['label_1']
y_train_3 = train_df['label_3']
y_train_4 = train_df['label_4']

x_train_2 = train_df[train_df['label_2'].notna()]                                              # Dropping NaN values for label 2
x_train_2.drop(labels, axis=1, inplace=True)
y_train_2 = train_df[train_df['label_2'].notna()]['label_2']

x_valid = valid_df.copy()
x_valid.drop(labels, axis=1, inplace=True)
y_valid_1 = valid_df['label_1']
y_valid_3 = valid_df['label_3']
y_valid_4 = valid_df['label_4']

x_valid_2 = valid_df[valid_df['label_2'].notna()]                                             # Dropping NaN values for label 2
x_valid_2.drop(labels, axis=1, inplace=True)
y_valid_2 = valid_df[valid_df['label_2'].notna()]['label_2']

x_test = test_df.copy()
x_test.drop(['ID'], axis=1, inplace=True)

"""*Principal Component Analysis* was used for all labels.

**Label 1** - with SVM model
"""

from sklearn.decomposition import PCA

new_feature_numbers = []
accuracy_values = []

n_c = 0.8
while(n_c<0.99):
  pca = PCA(n_components=n_c, svd_solver='full')
  pca.fit(x_train)

  x_train_trf = pd.DataFrame(pca.transform(x_train))
  x_valid_trf = pd.DataFrame(pca.transform(x_valid))
  new_feature_numbers.append(x_train_trf.shape[1])

  classifier = svm.SVC(kernel='linear')
  classifier.fit(x_train_trf,y_train_1)
  y_pred2 = classifier.predict(x_valid_trf)
  accuracy_values.append(accuracy_score(y_valid_1,y_pred2))
  n_c=n_c+0.02

i=0
while(i<len(accuracy_values)):
  print("Accuracy for "+ str(new_feature_numbers[i])+" features with "+str(0.8+0.02*i)+" n_components value :- "+str(accuracy_values[i]))
  i=i+1

"""Using 0.92 as the *n_components* value;"""

pca = PCA(n_components=0.92, svd_solver='full')
pca.fit(x_train)

x_train_trf = pd.DataFrame(pca.transform(x_train))
x_test_trf = pd.DataFrame(pca.transform(x_test))

classifier = svm.SVC(kernel='linear')
classifier.fit(x_train_trf,y_train_1)
y_pred = classifier.predict(x_test_trf)
pd.DataFrame(y_pred).to_csv("l1.csv")

"""**Label 2**"""

new_feature_numbers = []
accuracy_values = []

n_c = 0.8
while(n_c<0.99):
  pca = PCA(n_components=n_c, svd_solver='full')
  pca.fit(x_train_2)

  x_train_trf = pd.DataFrame(pca.transform(x_train_2))
  x_valid_trf = pd.DataFrame(pca.transform(x_valid_2))
  new_feature_numbers.append(x_train_trf.shape[1])

  classifier = svm.SVC(kernel='linear')
  classifier.fit(x_train_trf,y_train_2)
  y_pred2 = classifier.predict(x_valid_trf)
  accuracy_values.append(accuracy_score(y_valid_2,y_pred2))
  n_c=n_c+0.02

i=0
while(i<len(accuracy_values)):
  print("Accuracy for "+ str(new_feature_numbers[i])+" features with "+str(0.8+0.02*i)+" n_components value :- "+str(accuracy_values[i]))
  i=i+1

"""Using 0.92 as the n_components value;"""

pca = PCA(n_components=0.92, svd_solver='full')
pca.fit(x_train_2)

x_train_trf = pd.DataFrame(pca.transform(x_train_2))
x_test_trf = pd.DataFrame(pca.transform(x_test))

classifier = svm.SVC(kernel='linear')
classifier.fit(x_train_trf,y_train_2)
y_pred = classifier.predict(x_test_trf)
pd.DataFrame(y_pred).to_csv("l2.csv")

"""**Label 3**"""

from sklearn.decomposition import PCA

new_feature_numbers = []
accuracy_values = []

n_c = 0.8
while(n_c<0.99):
  pca = PCA(n_components=n_c, svd_solver='full')
  pca.fit(x_train)

  x_train_trf = pd.DataFrame(pca.transform(x_train))
  x_valid_trf = pd.DataFrame(pca.transform(x_valid))
  new_feature_numbers.append(x_train_trf.shape[1])

  classifier = svm.SVC(kernel='linear')
  classifier.fit(x_train_trf,y_train_3)
  y_pred2 = classifier.predict(x_valid_trf)
  accuracy_values.append(accuracy_score(y_valid_3,y_pred2))
  n_c=n_c+0.02

i=0
while(i<len(accuracy_values)):
  print("Accuracy for "+ str(new_feature_numbers[i])+" features with "+str(0.8+0.02*i)+" n_components value :- "+str(accuracy_values[i]))
  i=i+1

"""Using 0.96 as the n_components value;"""

pca = PCA(n_components=0.96, svd_solver='full')
pca.fit(x_train)

x_train_trf = pd.DataFrame(pca.transform(x_train))
x_test_trf = pd.DataFrame(pca.transform(x_test))

classifier = svm.SVC(kernel='linear')
classifier.fit(x_train_trf,y_train_3)
y_pred = classifier.predict(x_test_trf)
pd.DataFrame(y_pred).to_csv("l3.csv")

"""**Label 4**

An additional parameter, *class_weight* with the value *balanced* was used to resolve the issue with the uneven distribution of label values for SVM model.
"""

new_feature_numbers = []
accuracy_values = []

n_c = 0.8
while(n_c<0.99):
  pca = PCA(n_components=n_c, svd_solver='full')
  pca.fit(x_train)

  x_train_trf = pd.DataFrame(pca.transform(x_train))
  x_valid_trf = pd.DataFrame(pca.transform(x_valid))
  new_feature_numbers.append(x_train_trf.shape[1])

  classifier = svm.SVC(kernel='linear', class_weight="balanced")
  classifier.fit(x_train_trf,y_train_4)
  y_pred2 = classifier.predict(x_valid_trf)
  accuracy_values.append(accuracy_score(y_valid_4,y_pred2))
  n_c=n_c+0.02

i=0
while(i<len(accuracy_values)):
  print("Accuracy for "+ str(new_feature_numbers[i])+" features with "+str(0.8+0.02*i)+" n_components value :- "+str(accuracy_values[i]))
  i=i+1

"""Using 0.92 as the n_components value;"""

pca = PCA(n_components=0.92, svd_solver='full')
pca.fit(x_train)

x_train_trf = pd.DataFrame(pca.transform(x_train))
x_test_trf = pd.DataFrame(pca.transform(x_test))

classifier = svm.SVC(kernel='linear')
classifier.fit(x_train_trf,y_train_4)
y_pred = classifier.predict(x_test_trf)
pd.DataFrame(y_pred).to_csv("l4.csv")